{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb381f2b",
   "metadata": {},
   "source": [
    "# CA4\n",
    "\n",
    "Group 37\n",
    "\n",
    "Group members:\n",
    "* Jannicke Ådalen\n",
    "* Marcus Dalaker Figenschou\n",
    "* Rikke Sellevold Vegstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927caf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn import ensemble as ens\n",
    "from sklearn import metrics as met\n",
    "from sklearn import model_selection as msel\n",
    "from sklearn import pipeline as pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6a6653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the styles of plots so that they have same styling throughout\n",
    "sns.set_style(\"white\")\n",
    "plt.rcParams[\"xtick.labelsize\"] = 16\n",
    "plt.rcParams[\"ytick.labelsize\"] = 16\n",
    "plt.rcParams[\"axes.spines.left\"] = False\n",
    "plt.rcParams[\"axes.spines.right\"] = False\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "\n",
    "# Set working directory\n",
    "if \"CA4\" in os.getcwd():\n",
    "    os.chdir(\"..\")  # Go up one level if we're in CA3\n",
    "\n",
    "print(f\"Working directory now: {os.getcwd()}\")\n",
    "\n",
    "# Load data\n",
    "train_path = os.path.join(\"CA4\", \"assets\", \"train.csv\")\n",
    "test_path = os.path.join(\"CA4\", \"assets\", \"test.csv\")\n",
    "\n",
    "# Load data\n",
    "# 1. Load data\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d9c20",
   "metadata": {},
   "source": [
    "# Data inspection and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---TRAIN DATA---\")\n",
    "train_df.info()\n",
    "print(\"---TEST DATA---\")\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ab859",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "A lot of metadata in this dataset (data that isnt physical properties) that is just used to organize the data for the future by astronomers. Seems also that u might be missing values since it has fewer entries and shows up as non-null, will investigate after metadata removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6bd9ef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "features = [\"u\", \"g\", \"r\", \"i\", \"z\", \"redshift\", \"class\"]\n",
    "# Keeping everything except metadata columns\n",
    "train_df = train_df[features]\n",
    "print(train_df.head())\n",
    "\n",
    "# Modifying test_data\n",
    "test_df = test_df[features[:-1]]\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb505172",
   "metadata": {
    "title": "EDA - Data description"
   },
   "outputs": [],
   "source": [
    "print(\"--- Train Data ---\")\n",
    "print(train_df.describe())\n",
    "\n",
    "print(\"--- Test Data ---\")\n",
    "print(test_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8225d03f",
   "metadata": {},
   "source": [
    "Two things to see here. From the .info we can see that there are fewer observations of u than the rest and we have a min value -9999 for u, g and z which only shows up in train data but not in test data. But in our original data inspection we found no missing values, so this might indicate placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a06b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79efef",
   "metadata": {},
   "source": [
    "\n",
    "We can now see that u is actually missing 362 values. Because of our datasize of 80k, we can comfortably remove 362 samples from our dataset of 80k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d63d99",
   "metadata": {
    "title": "dropping na"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()\n",
    "\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a569d6",
   "metadata": {
    "title": "Data description after na removal"
   },
   "outputs": [],
   "source": [
    "print(\"--- Train Data ---\")\n",
    "train_df.info()\n",
    "\n",
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd9614a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "There are still values with -9999, will investigate this further. This might be placeholder values, lets check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1808bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_counts = (train_df.drop(columns=[\"class\"]) == -9999).sum()\n",
    "print(\"Negative value counts:\")\n",
    "print(negative_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afeb6e2",
   "metadata": {},
   "source": [
    "Yes there is exactly one value with -9999 in the u, g and z columns. Will remove these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda3d5b",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Replacing them with nan, and checking that they are now removed"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.replace(-9999, np.nan)\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce8173",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "New eda on cleaned up data"
   },
   "outputs": [],
   "source": [
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73bf436",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Checking class distribution"
   },
   "outputs": [],
   "source": [
    "print(train_df[\"class\"].value_counts(normalize=True).mul(100).round(2).astype(str) + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf852a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The class distribution is heavily skewed towards galaxies. Roghly 60 % of the dataset are galaxies. We can balance this out by downsampling galaxies, we will do this later just before we split our data into test/val."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470b6519",
   "metadata": {},
   "source": [
    "# Data plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc9f605",
   "metadata": {},
   "source": [
    "Before we go forward with plotting we will map galaxy, quasars and stars to their given values for submission. We will keep the original labels since this is more descriptive when we plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0587c3ee",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Encode class labels"
   },
   "outputs": [],
   "source": [
    "class_mapping = {\"GALAXY\": 0, \"QSO\": 1, \"STAR\": 2}\n",
    "# Keep original text labels for plotting\n",
    "train_df[\"class_labels\"] = train_df[\"class\"]\n",
    "# Replaceing with numeric codes\n",
    "train_df[\"class\"] = train_df[\"class\"].map(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37bdbe9",
   "metadata": {},
   "source": [
    "Since there is ~ 79k samples in our dataset we choose to extract a random sample of 5000. This is statistically enought to visualize the data distribution without needing to visualize ~ 79k samples which for some plots takes a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0907f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_df.columns.drop([\"class\", \"class_labels\"])\n",
    "classes = train_df[\"class_labels\"].unique()\n",
    "\n",
    "sample_size = 5000\n",
    "plot_df = train_df.columns.drop([\"class\"])\n",
    "plot_df_sample = train_df.sample(n=sample_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3172a6",
   "metadata": {},
   "source": [
    "## Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73933496",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d83b37d",
   "metadata": {
    "title": "EDA - Boxplot of raw data by feature"
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    sns.boxplot(\n",
    "        x=\"class_labels\",\n",
    "        y=feature,\n",
    "        data=plot_df_sample,\n",
    "        hue=\"class_labels\",\n",
    "        palette=\"Set1\",\n",
    "        ax=axes[i],\n",
    "    )\n",
    "    axes[i].set_title(f\"Boxplot of {feature}\")\n",
    "    axes[i].set_xlabel(\"\")\n",
    "    axes[i].set_ylabel(\"\")\n",
    "\n",
    "    axes[i].tick_params(axis=\"x\", labelsize=14)\n",
    "    axes[i].patch.set_edgecolor(\"black\")\n",
    "    axes[i].patch.set_linewidth(1)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.2)\n",
    "plt.savefig(\"boxplot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb698ba",
   "metadata": {},
   "source": [
    "\n",
    "Based on the boxplot quasars has the most outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e783435",
   "metadata": {},
   "source": [
    "## Violinplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132ff4c",
   "metadata": {
    "title": "EDA - Distribution plots"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "for i, feature in enumerate(features):\n",
    "    sns.violinplot(\n",
    "        data=plot_df_sample,\n",
    "        x=\"class_labels\",\n",
    "        y=feature,\n",
    "        hue=\"class_labels\",\n",
    "        palette=\"Set1\",\n",
    "        ax=axes[i],\n",
    "        alpha=0.4,\n",
    "        orient=\"v\",\n",
    "    )\n",
    "    axes[i].set_title(f\"Violinplot of {feature}\")\n",
    "    axes[i].set_xlabel(\"\")\n",
    "    axes[i].set_ylabel(\"\")\n",
    "    axes[i].tick_params(axis=\"x\", labelsize=14)\n",
    "    axes[i].patch.set_edgecolor(\"black\")\n",
    "    axes[i].patch.set_linewidth(1)\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d04b44",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The violinplot shows that galaxy is biomodal for all features. Star is somewhat biomodal for features: g, r and i. Quasars is only unimodal.\n",
    "\n",
    "It is interesting that galaxies are biomodal for all features, this might be due to galaxies come in different sizes and shapes, and also have varying degrees of luminosity.\n",
    "\n",
    "That some features show stars as biomodal is also to be expected as stars vary in size and shape, but we did believe that this effect should be stronger than it is .\n",
    "\n",
    "The quasars are unimodal for all features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10855640",
   "metadata": {},
   "source": [
    "## Feature Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa07a68",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Corr matrix for all classes"
   },
   "outputs": [],
   "source": [
    "samples = [plot_df_sample[plot_df_sample[\"class_labels\"] == cls][features] for cls in classes]\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot correlation matrix for each class\n",
    "for i, (df, title) in enumerate(zip(samples, classes)):\n",
    "    correlation_matrix = df.corr()\n",
    "    sns.heatmap(\n",
    "        correlation_matrix,\n",
    "        annot=True,\n",
    "        cmap=\"coolwarm\",\n",
    "        fmt=\".2f\",\n",
    "        ax=axes[i],\n",
    "        square=True,\n",
    "        vmin=-1.0,\n",
    "        vmax=1.0,\n",
    "    )\n",
    "    axes[i].set_title(f\"Correlation Matrix - {title} (n={len(df)})\")\n",
    "fig.delaxes(axes[3])\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=90)\n",
    "fig.subplots_adjust(hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71bd84e",
   "metadata": {},
   "source": [
    "From the correlation matrix we can see that redshift will be our most important feature when it comes to seperating the three categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6d0aa",
   "metadata": {},
   "source": [
    "# Creating New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5e3af",
   "metadata": {},
   "source": [
    "Now that we have a balanced dataset, we can create new features based on color indices—a method astronomers use to assess the light intensity of celestial objects. We must also remember to add these features to our original training data and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665abec3",
   "metadata": {},
   "source": [
    "We can group the filters based on thier photometric system like this:\n",
    "\n",
    "Visible light:\n",
    "- g(green filter)\n",
    "- r(red filter)\n",
    "\n",
    "Ultraviolet light:\n",
    "- u(ultraviolet)\n",
    "\n",
    "Infrared spectrum:\n",
    "- i(near infrared)\n",
    "- z(infrared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770a7fd",
   "metadata": {
    "title": "Feature engineering"
   },
   "outputs": [],
   "source": [
    "# creating a definition we can use later to create new feature datasets\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"Adding astronomical color indices to the dataframe.\"\"\"\n",
    "    df_features = df.copy()\n",
    "\n",
    "    df_features[\"r-z\"] = df_features[\"r\"] - df_features[\"z\"]\n",
    "    df_features[\"g-r\"] = df_features[\"g\"] - df_features[\"r\"]\n",
    "    df_features[\"u-r\"] = df_features[\"u\"] - df_features[\"r\"]\n",
    "    df_features[\"i-z\"] = df_features[\"i\"] - df_features[\"z\"]\n",
    "    df_features[\"i-r\"] = df_features[\"i\"] - df_features[\"r\"]\n",
    "    df_features[\"u-g\"] = df_features[\"u\"] - df_features[\"g\"]\n",
    "    df_features[\"g-i\"] = df_features[\"g\"] - df_features[\"i\"]\n",
    "\n",
    "    # redshift is the feature that seems to seperate the classes best so we will create some more\n",
    "\n",
    "    df_features[\"redshift_squared\"] = df_features[\"redshift\"] ** 2\n",
    "    df_features[\"log_redshift\"] = np.log1p(df_features[\"redshift\"])\n",
    "\n",
    "    # Simple multiplication interactions\n",
    "    df_features[\"redshift_u\"] = df_features[\"redshift\"] * df_features[\"u\"]\n",
    "\n",
    "    # We try to create a brightness feature here to\n",
    "    df_features[\"spectral_contrast\"] = df_features[[\"u-g\", \"g-r\", \"i-r\", \"i-z\"]].max(axis=1) - df_features[\n",
    "        [\"u-g\", \"g-r\", \"i-r\", \"i-z\"]\n",
    "    ].min(axis=1)\n",
    "\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c19e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = feature_engineering(train_df)\n",
    "test_df = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f501fc",
   "metadata": {},
   "source": [
    "We ran first a random forest model with all the features above and then we plotted the cumuluative feature importances and chose to keep the features that had ~95% relevance for our model, these are the features we we ended up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef31ff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = [\n",
    "    \"redshift_squared\",\n",
    "    \"log_redshift\",\n",
    "    \"redshift\",\n",
    "    \"redshift_u\",\n",
    "    \"g-i\",\n",
    "    \"g-r\",\n",
    "    \"spectral_contrast\",\n",
    "    \"i-r\",\n",
    "    \"r-z\",\n",
    "    \"u-r\",\n",
    "    \"g\",\n",
    "    \"class\",\n",
    "    \"class_labels\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6983a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_rf = train_df[top_features]\n",
    "print(train_df_rf)\n",
    "\n",
    "test_df_rf = test_df[top_features[:-2]]\n",
    "print(test_df_rf.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd209a9",
   "metadata": {},
   "source": [
    "# Training the models\n",
    "Since we are working with three categorical target values and a large dataset we have chosen to go for the following models:\n",
    "- Random Forest Classifier\n",
    "- Logistic Regression model\n",
    "- SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f6994",
   "metadata": {
    "title": "Creating test split data set with sample size for faster training"
   },
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=[\"class\", \"class_labels\"])\n",
    "y = train_df[\"class\"]\n",
    "\n",
    "# Splitting the data into train and test data with a 60/40 split\n",
    "X_train, X_test, y_train, y_test = msel.train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5daed71",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97bc7f6",
   "metadata": {},
   "source": [
    "Random forest is especially good for unbalanced data and multi category classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e6737",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = ens.RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "rf_pipe = pipe.Pipeline([(\"random_forest\", random_forest)])\n",
    "\n",
    "\n",
    "# Parameter distributions for random search\n",
    "param_grid_rf = {\n",
    "    \"random_forest__n_estimators\": [100, 200, 300, 400, 500],\n",
    "    \"random_forest__max_depth\": [5, 10, 15, 20, 25, None],\n",
    "    \"random_forest__min_samples_split\": [2, 5, 10, 15, 20],\n",
    "    \"random_forest__max_features\": [\"sqrt\", \"log2\"],\n",
    "    \"random_forest__criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "}\n",
    "\n",
    "# Since data is unbalanced we will use stratifiedkfold\n",
    "stratified_cv = msel.StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# We will use randomizedsearchCV since we are doing hyperparamter tuning\n",
    "rf_search = msel.RandomizedSearchCV(\n",
    "    rf_pipe,\n",
    "    param_distributions=param_grid_rf,\n",
    "    n_iter=100,\n",
    "    # Using StratifiedKFold\n",
    "    cv=stratified_cv,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"f1_macro\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "rf_search.fit(X_train, y_train)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % rf_search.best_score_)\n",
    "print(rf_search.best_params_)\n",
    "\n",
    "\n",
    "# Print best results\n",
    "print(\"Best parameter (CV score=%0.3f):\" % rf_search.best_score_)\n",
    "print(rf_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d889225",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Evaluate on test set"
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "best_rf_model = rf_search.best_estimator_\n",
    "y_test_pred_rf = best_rf_model.score(X_test, y_test)\n",
    "print(f\"Test set score with best model: {y_test_pred_rf:.3f}\")\n",
    "print(best_rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a79e5",
   "metadata": {
    "title": "Detailed evaluation on test set from sample"
   },
   "outputs": [],
   "source": [
    "# Get predictions on the sample test set\n",
    "y_pred_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report on Sample Test Set:\")\n",
    "print(met.classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Create confusion matrix\n",
    "print(\"Confusion Matrix on Sample Test Set:\")\n",
    "conf_matrix = met.confusion_matrix(y_test, y_pred_rf)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    conf_matrix,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=best_rf_model.classes_,\n",
    "    yticklabels=best_rf_model.classes_,\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (Sample Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad67e2",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Extract best parameters from the sample model"
   },
   "outputs": [],
   "source": [
    "# Extract best parameters from the best model\n",
    "best_n_estimators = rf_search.best_params_[\"random_forest__n_estimators\"]\n",
    "best_max_depth = rf_search.best_params_[\"random_forest__max_depth\"]\n",
    "best_min_samples_split = rf_search.best_params_[\"random_forest__min_samples_split\"]\n",
    "best_max_features = rf_search.best_params_[\"random_forest__max_features\"]\n",
    "best_criterion = rf_search.best_params_[\"random_forest__criterion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e64e795",
   "metadata": {
    "title": "Create a pipeline with the best parameters"
   },
   "outputs": [],
   "source": [
    "print(\"Training final model on the entire dataset...\")\n",
    "\n",
    "# Prepare full data\n",
    "X_full = train_df.drop(columns=[\"class_labels\", \"class\"])\n",
    "y_full = train_df[\"class\"]\n",
    "\n",
    "\n",
    "final_rf = ens.RandomForestClassifier(\n",
    "    n_estimators=best_n_estimators,\n",
    "    max_features=best_max_features,\n",
    "    max_depth=best_max_depth,\n",
    "    criterion=best_criterion,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    ")\n",
    "\n",
    "\n",
    "# Create the final pipeline with scaling\n",
    "final_rf_pipeline = pipe.Pipeline([(\"random_forest\", final_rf)])\n",
    "\n",
    "# Train the pipeline on unscaled data - pipeline handles scaling internally\n",
    "final_rf_pipeline.fit(X_full, y_full)\n",
    "\n",
    "print(f\"Final rf model trained on {len(X_full)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b8228",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f5fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_rf = final_rf_pipeline.predict(test_df)\n",
    "\n",
    "# Create the final dataframe with numeric class predictions\n",
    "y_test_rf = pd.DataFrame(y_test_rf, columns=[\"class\"])\n",
    "y_test_rf.index.name = \"ID\"\n",
    "\n",
    "# Add file path with appropriate naming related to model parameters\n",
    "base_dir = os.path.join(\"CA4\", \"results\")\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "filename = f\"random_forest_model.csv\"  # noqa: F541\n",
    "file_path = os.path.join(base_dir, filename)\n",
    "\n",
    "# Save to CSV\n",
    "y_test_rf[[\"class\"]].to_csv(file_path)\n",
    "print(f\"Saved rf submission to {file_path}\")\n",
    "\n",
    "\n",
    "def test_long_line():\n",
    "    very_long_string = \"This is an extremely long string that definitely exceeds the 120 character limit that you've set in your Ruff configuration for the Zed editor and should trigger formatting or linting warnings when you save this file or run Ruff manually on it.\"\n",
    "\n",
    "    # Here's another long line with a list comprehension that exceeds 120 characters\n",
    "    very_long_list = [\n",
    "        x\n",
    "        for x in range(1000)\n",
    "        if x % 2 == 0 and x % 3 == 0 and x % 5 == 0 and x % 7 == 0 and x % 11 == 0 and str(x).startswith(\"1\")\n",
    "    ]\n",
    "\n",
    "    return very_long_string, very_long_list"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "DAT200 Environment",
   "language": "python",
   "name": "dat200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
